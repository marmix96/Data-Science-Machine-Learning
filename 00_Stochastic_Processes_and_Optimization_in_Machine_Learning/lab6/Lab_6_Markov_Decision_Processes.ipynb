{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XLZdEbAy2jfr"
   },
   "source": [
    "<h1><b>Markov Decision Processes</h1></b>\n",
    "<p align=\"justify\">Στη συγκεκριμένη άσκηση θα μελετήσετε τους αλγορίθμους <i>Policy Iteration</i> και <i>Value Iteration</i>, καθώς και θα εξοικειωθείτε με βασικές έννοιες των <i>Markov Decision Processes</i>. Οι αλγόριθμοι <i>Policy Iteration</i> και <i>Value Iteration</i> είναι από τους βασικούς αλγορίθμους δυναμικού προγραμματισμού που χρησιμοποιούνται για την επίλυση της εξίσωσης <i>Bellman</i> σε <i>Markov Decision Processes</i>.</p> \n",
    "<p align=\"justify\">Το πρόβλημα που θα μελετήσετε είναι αυτό της παγωμένης λίμνης (Frozen Lake) με μέγεθος πλέγματος 8 x 8.</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6VsUV229__zO"
   },
   "source": [
    "<h2><b>Εξοικείωση με τη βιβλιοθήκη <i>Gym</i></b></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OM8ivgOJAg_H"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "from gym import wrappers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_puV3ugeAnbU"
   },
   "source": [
    "Με την παρακάτω εντολή, ορίζετε το πρόβλημα που θα μελετηθεί:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ep-MvIUCAxT8"
   },
   "outputs": [],
   "source": [
    "env_name = 'FrozenLake8x8-v0'\n",
    "env = gym.make(env_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uBKBXJDUBRUh"
   },
   "source": [
    "Με τις παρακάτω εντολές, θα επαναφέρετε τον Agent στην αρχική του θέση και θα οπτικοποιήσετε το πλέγμα και τη θέση του Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p6lqbG9zBgdi"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FX2res4JBlYb"
   },
   "source": [
    "Με τις παρακάτω εντολές, ορίζετε την επόμενη ενέργεια με τυχαίο τρόπο και ο Agent κάνει ένα βήμα."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Gq7q944YBx0Q"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------\n",
      "  (Up)\n",
      "\u001b[41mS\u001b[0mFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "-------------\n",
      "  (Right)\n",
      "SFFFFFFF\n",
      "\u001b[41mF\u001b[0mFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "-------------\n",
      "  (Down)\n",
      "SFFFFFFF\n",
      "\u001b[41mF\u001b[0mFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "-------------\n",
      "  (Up)\n",
      "SFFFFFFF\n",
      "F\u001b[41mF\u001b[0mFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "-------------\n",
      "  (Up)\n",
      "S\u001b[41mF\u001b[0mFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "-------------\n",
      "  (Up)\n",
      "\u001b[41mS\u001b[0mFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "-------------\n",
      "  (Up)\n",
      "\u001b[41mS\u001b[0mFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "-------------\n",
      "  (Down)\n",
      "S\u001b[41mF\u001b[0mFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "-------------\n",
      "  (Right)\n",
      "S\u001b[41mF\u001b[0mFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "-------------\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    next_action = env.action_space.sample()\n",
    "    print(\"-------------\")\n",
    "    env.step(next_action)\n",
    "    env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mV4A7lsLB54y"
   },
   "source": [
    "Να εκτελέσετε αρκετές φορές τις τελευταίες εντολές και να παρατηρήσετε κάθε φορά την ενέργεια που ζητείται από τον agent να εκτελέσει και την ενέργεια που αυτός πραγματοποιεί. Πραγματοποιεί πάντοτε ο agent την κίνηση που του ζητείται; Είναι ντετερμινιστικές ή στοχαστικές οι κινήσεις του agent;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ακολούθως παρουσιάζονται 10 κινήσεις του agent, μαζί με το αντίστοιχο action που απαιτείται να πραγματοποιήσει. Είναι φανερό, όμως, ότι σε ορισμένες περιπτώσεις ο agent εν τέλει μετέβαινε σε κάποιο άλλο state, διαφορετικό από εκείνο που του υποδεικνύει το αντίστοιχο action. Κατ΄ αυτόν τον τρόπο, φανερώνεται ενά πολύ σημαντικό χαρακτηριστικό των real-world environments, κατά το οποίο οι μεταβάσεις από μία κατάσταση του MDP σε κάποια άλλη, δεδομένου ενός action, γίνεται πιθανοτικά. Ως αποτέλεσμα επομένως, μπορούμε να συμπεράνουμε ότι η μελλονική κατάσταση στην οποία θα βρεθεί ο agent δεν εξαρτάται αποκλειστικά από το ληφθέν action. Δηλαδή, οι κινήσεις του agent είναι στοχαστικές."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PAL4we3gDV_J"
   },
   "source": [
    "<h2><b>Ερωτήσεις</b></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zQKm4VAUChi1"
   },
   "source": [
    "<ul>\n",
    "<li>Μελετώντας <a href=\"https://gym.openai.com/envs/FrozenLake-v0/\">αυτό</a> και βασισμένοι στα συμπεράσματα του προηγούμενου ερωτήματος να περιγράψετε σύντομα το πρόβλημα της παγωμένης λίμνης ως πρόβλημα βελτιστοποίησης. Ποιος είναι ο στόχος του agent;</li>\n",
    "<li>Να διατυπώσετε την ιδιότητα <i>Markov</i>. Πώς απλοποιεί η ιδιότητα αυτή τη μελέτη του συγκεκριμένου προβλήματος;</li>\n",
    "<li>Να περιγράψετε σύντομα τους αλγορίθμους <i>Policy Iteration</i> και <i>Value Iteration</i>, δίνοντας έμφαση στο διαφορετικό τρόπο με τον οποίο προσεγγίζουν την επίλυση του προβλήματος. Είναι εγγυημένο ότι οι δύο αλγόριθμοι θα συγκλίνουν στη βέλτιστη πολιτική; Αν ναι, οδηγούν σε ίδια ή διαφορετική βέλτιστη πολιτική;</li>\n",
    "<li>Να εκτελέσετε τα προγράμματα που σας δίνονται, τα οποία επιλύουν το\n",
    "πρόβλημα της παγωμένης λίμνης, χρησιμοποιώντας τους αλγορίθμους <i>Policy\n",
    "Iteration</i> και <i>Value Iteration</i> αντίστοιχα. Ποια μέθοδος συγκλίνει στη βέλτιστη λύση σε λιγότερα βήματα; Τι συμπέρασμα βγάζετε; Να ελέγξετε το χρόνο εκτέλεσης του κάθε προγράμματος, χρησιμοποιώντας την εντολή <i>time</i>. Τι συμπέρασμα βγάζετε ως προς την πολυπλοκότητα του κάθε αλγορίθμου;</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Αρχικά η λύση του συγκεκριμένου προβλήματος, ανάγεται στην επίλυση ενός MDP, δηλαδή στην εύρεση ενός διανύσματος από policies, $\\mu_i, i = 1, 2, ..., N$, έστω $\\boldsymbol{\\mu} = (\\mu_0, ..., \\mu_{N-1})$. Επιπλέον κάθε ένα MDP (πεπερασμένου ορίζοντα) χαρακτηρίζεται από: \n",
    "\n",
    " * $\\mathcal{X}$: state space, για το συγκεκριμένο πρόβλημα έχουμε $\\mathcal{X} = \\{S,F,H,G\\}$, όπου \"S\": start, \"F\": frozen lake, \"H\": hole, \"G\": final goal,\n",
    "  * $\\mathcal{U}$: action space, για το συγκεκριμένο πρόβλημα έχουμε $\\mathcal{U} = \\{Left, Down, Right, Up\\}$,\n",
    "  * $P_{i,j}(u, k) = \\mathbb{P}(x_{k+1} = j|x_k = i, u_k = u), \\forall i,j \\in \\mathcal{X}$,\n",
    "  * $g(i,u,j), g_{N}(i)$: one-stage cost και terminal cost, αντίστοιχα, με βάση τα οποία ζημειώνεται ο agent. <br> Η βέλτιστη, λοιπόν, επίλυση του πρβλήματος MDP, γράφεται σαν πρόβλημα βελτιστοποίησης ως εξής: $J^{*}_{\\boldsymbol{\\mu}}(x) = \\min_{\\boldsymbol{\\mu}} \\mathbb{E}_{\\boldsymbol{\\mu}} \\left\\{\\sum_{n = 0}^{N-1} g_n(x_n, \\mu_n(x_n), x_{n+1}) + g_N(x_N) | x_0 = x \\right\\}$ όπου $\\boldsymbol{\\mu}^*$ το διάνυσμα βέλτιστων πολιτικών, με βάση το οποίο επιτυγχάνεται ελαχιστοποίηση της συνάρτησης κόστους $J_{\\boldsymbol{\\mu}}$. Προφανώς στόχος του agent είναι ουσιαστικά η επίλυση υφιστάμενου προβλήματος MDP, ώστε να μεταβεί από το σημείο εκκίνησης στο σημείο τερματισμού με το μικρότερο δυνατό κόστος, μετρούμενο ως πρός μία συνάρτηση κόστους $g(\\cdot)$.\n",
    "  \n",
    "- Η ιδιότητα Markov διατυπώνεται ως εξής $$ \\mathbb{P}[X_{t+1} | X_1,\\dots, X_t] = \\mathbb{P}[X_{t+1} | X_t]$$ \n",
    "Με απλά λόγια, η πιθανότητα να μεταβούμε σε κάποια κατάσταση την επόμενη χρονική στιγμή δεδομένου ότι ως τώρα έχουμε ακολουθήσει ένα συγκεκριμένο μονοπάτι δεν εξαρτάται από ολόκληρο το μονοπάτι παρά μόνο από την θέση μας αυτή τη στιγμή. Οπότε απλοποιεί αρκετά το πρόβλημά μας καθώς η πιθανότητα να φτάσουμε στο G εξαρτάται μονό από την τρέχουσα θέση μας.\n",
    "\n",
    "- Οι αλγόριθμοι Policy και Value Iteration αποτελούν κλασικές αριθμητικές μεθόδους για την εύρεση της βέλτιστης πολιτικής που χαρακτηρίζει το δοθέν MDP. Όσον αφορά τον αλγόριθμο Policy Iteration, πρόκειται για μια επαναληπτική μέθοδο, η οποία υπολογίζει και βελτιώνει την υπάρχουσα πολιτική σε κάθε επανάληψη. Δεδομένου ότι οι χώροι των ενεργειών και των καταστάσεων είναι πεπερασμένοι, είναι φανερό ότι το πλήθος των πιθανών στάσιμων πολιτικών είναι και εκείνο πεπερασμένο, και συγκεκριμένα $|\\mathcal{X}|^{|\\mathcal{U}|}$. Ως αποτέλεσμα, η συγκεκριμένη μέθοδος στοχεύει στην αναζήτηση εντός του χώρου των στάσιμων πολιτικών. Ξεκινώντας από μία αρχική πολιτική $\\boldsymbol{\\mu}_0$, η μέθοδος αποτελείται από δύο επιμέρους φάσεις:\n",
    "  * Βελτίωση της τρέχουσας πολιτικής, όπου πραγματοποιείται ο υπολογισμός της επόμενης πολιτικής που θα επισκεφτεί η μέθοδος, δεδομένης της τωρινής. Η νέα πολιτική υπολογίζεται ως εξής:\n",
    "  $\\mu_{n+1}(i) = argmin_{a \\in A} \\left\\{c(i, a) + \\gamma \\sum_j P_{ij}(a) J_{\\mu_{n}}(i) \\right\\}, \\forall i \\in \\mathcal{X}$\n",
    "  *  Αξιολόγηση της προκύπτουσας πολιτικής, όπου δοθείσης πλέον της νέας πολιτικής που προέκυψε, υπολογίζεται το discounted συσσωρευτικό κόστος που σχετίζεται με την συγκεκριμένη πολιτική, λύνοντας το ακόλουθο γραμμικό σύστημα:\n",
    "  $J_{\\mu_{n+1}}(i) = c(i, \\mu_{n+1}(i)) + \\gamma \\sum_j P_{ij}(\\mu_{n+1}(i)) J_{\\mu_{n+1}}(i), \\forall i \\in \\mathcal{X}$ <br>\n",
    "  \n",
    "  Όπως είναι αναμενόμενο, η συγκεκριμένη μέθος συγκλίνει σε πεπερασμένο αριθμό βημάτων στην βέλτιστη στάσιμη κατανομή, δεδομένου ότι το πλήθος αυτών είναι και εκείνο πεπερασμένο. <br>\n",
    "  Σχετικά με την μέθοδο Value Iteration, παρομοίως πρόκειται για μία επαναληπτική μέθοδο, όπου επιλέγοντας έναν ικανοποιητικά μεγάλο αριθμό επαναλήψεων Ν και αρχικοποιώντας τo $J_0(i), \\forall i \\in \\mathcal{X}$, υπολογίζεται επαναληπτικά η τιμή του $J_{n+1}$, δοθέντος του $J_n$, ως εξής: <br> $J_{n+1}(i) = \\min_{a \\in A} \\left\\{ c(i,a) + \\gamma \\sum_j P_{ij}(a)J_n(j) \\right\\}$, ενώ επιπλέον υπολογίζεται το ,<br>\n",
    "$\\mu_{n+1}(i) = argmin_{a \\in A} J_{n+1}(i), \\forall i \\in \\mathcal{X}$. <br> Εν τέλει ο αλγόριθμος καταλήγει σε κάποια πολιτική $\\mu_N$, είτε εξαντλώντας έναν προκαθορισμένο αριθμό βημάτων, ή όταν σε διαδοχικές επαναλήψεις η διαφορά των τιμών $J_n$ και $J_{n+1}$ διαγέρουν λιγότερο από κάποιο tolerance $\\epsilon$, ή με κάποιο άλλο κριτήριο προκειμένου να εξασφαλιστεί ότι ο αλγόριθμος σταματάει όταν η μέθοδος έχει πια συγκλίνει (η μέθοδος συγκλίνει γεωμετρικά στην βέλτιστη $J^*$, η οποία ικανοποιεί την εξίσωση Bellman).\n",
    "Έχοντας περιγράψει συνοπτικά τις δύο διαφορετικές μεθόδους, παρατηρούμε ότι διαφέρουν στο τρόπο με τον οποίο επιλέγουν να λύσουν τα προβλήματα MDPs. Ειδικότερα, η λογική του Policy Interation είναι ότι ξεκινώντας από μία τυχαία policy, από τον χώρο των στάσιμων πολιτικών, ακολουθεί η φάση του Policy Improvement, ενώ έπεται η φάση του Policy Evalution. Στην περίπτωση αυτή, αποκλειστικός στόχος είναι η εύρεση της βέλτιστης πολιτικής, γεγονός που καθιστά εφικτή την σύγκλιση της μεθόδου, πολύ πρίν την σύγκλιση της value function (βλ. Value Iteration αλγόριθμος). Συνήθως η συγκεκριμένη μέθοδος απαιτεί αισθητά λιγότερα βήματα για να συγκλίνει σε σχέση με την Value Iteration, η οποία συζητάται ακολούθως. Ειδικότερα, όσον αφορά την Value Iteration, ξεκινώντας από ένα τυχαία αρχικοποιημένο $J_0(i), \\forall i \\in \\mathcal{X}$, επιλέγεται το βέλτιστο $J_n$ (θεωρώντας ότι είμαστε στο n-οστό βήμα), ενώ έπειτα, με βάση εκείνο, προκείπτει η βέλτιστη πολιτική $\\mu_n(i), \\forall i \\in \\mathcal{X}$. Όταν ο αλγόριθμος ολοκληρωθεί (έχει συγκλίνει), κρατά την πίο πρόσφατη επισκευφθείσα πολιτική, ως στάσιμη. Τέλος και οι δύο μέθοδοι συγκλίνουν στην βέλτιστη πολιτική, με την μόνη διαφορά ότι ο Policy Iteration συγκλίνει σε πεπερασμένο αριθμό βημάτων, δηλαδή στην πολιτική με το μικρότερο δυνατό συσσωρευτικό κόστος, η οποία δεν είναι αναγκαστικό να είναι είναι μοναδική. Κάτι τέτοιο συνεπάγεται ότι ενδεχομένως να οδηγούν σε δύο διαφορετικές πολιτικές, οι οποίες όμως θα είναι και οι δύο βέλτιστες. \n",
    "\n",
    "- Παρατηρούμε ότι ο policy iteration συνέκλινε λίγο πιο γρήγορα σε χρόνο αλλά με πολύ λιγότερα βήματα. Συνεπώς συμπεραίνουμε ότι ο policy iteration χρειάζεται λιγότερες αλλά πιο χρονοβόρες επαναλήψεις ενώ value iteration περισσότερες αλλά λιγότερο χρονοβόρες. Αναφορικά με την πολυπλοκότητα, το κεντρικό κομμάτι των loop διαφέρει στους δύο αλγοριθμους: στον policy iteration για κάθε state κοιτάμε όλα τα states ενώ στον value iteration για κάθε action κοιτάμε όλα τα states οπότε έχουμε $O(|S|^3) \\text{ vs } O(|S|^2|A|)$ λαμβάνοντας υπόψιν όλο το loop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "S6mci5P4HJ_1"
   },
   "source": [
    "<h2><b>Policy Iteration</b></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_43MjfJ9G8v7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy-Iteration converged at step 8.\n",
      "Average scores =  1.0\n",
      "Time = 1.8960275650024414\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Solving FrozenLake8x8 environment using Policy iteration.\n",
    "Author : Moustafa Alzantot (malzantot@ucla.edu)\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import gym\n",
    "from gym import wrappers\n",
    "from time import time\n",
    "\n",
    "def run_episode(env, policy, gamma = 1.0, render = False):\n",
    "    \"\"\" Runs an episode and return the total reward \"\"\"\n",
    "    obs = env.reset()\n",
    "    total_reward = 0\n",
    "    step_idx = 0\n",
    "    while True:\n",
    "        if render:\n",
    "            env.render()\n",
    "        obs, reward, done , _ = env.step(int(policy[obs]))\n",
    "        total_reward += (gamma ** step_idx * reward)\n",
    "        step_idx += 1\n",
    "        if done:\n",
    "            break\n",
    "    return total_reward\n",
    "\n",
    "\n",
    "def evaluate_policy(env, policy, gamma = 1.0, n = 100):\n",
    "    scores = [run_episode(env, policy, gamma, False) for _ in range(n)]\n",
    "    return np.mean(scores)\n",
    "\n",
    "def extract_policy(v, gamma = 1.0):\n",
    "    \"\"\" Extract the policy given a value-function \"\"\"\n",
    "    policy = np.zeros(env.nS)\n",
    "    for s in range(env.nS):\n",
    "        q_sa = np.zeros(env.nA)\n",
    "        for a in range(env.nA):\n",
    "            q_sa[a] = sum([p * (r + gamma * v[s_]) for p, s_, r, _ in  env.P[s][a]])\n",
    "        policy[s] = np.argmax(q_sa)\n",
    "    return policy\n",
    "\n",
    "def compute_policy_v(env, policy, gamma=1.0):\n",
    "    \"\"\" Iteratively evaluate the value-function under policy.\n",
    "    Alternatively, we could formulate a set of linear equations in iterms of v[s] \n",
    "    and solve them to find the value function.\n",
    "    \"\"\"\n",
    "    v = np.zeros(env.nS)\n",
    "    eps = 1e-10\n",
    "    while True:\n",
    "        prev_v = np.copy(v)\n",
    "        for s in range(env.nS):\n",
    "            policy_a = policy[s]\n",
    "            v[s] = sum([p * (r + gamma * prev_v[s_]) for p, s_, r, _ in env.P[s][policy_a]])\n",
    "        if (np.sum((np.fabs(prev_v - v))) <= eps):\n",
    "            # value converged\n",
    "            break\n",
    "    return v\n",
    "\n",
    "def policy_iteration(env, gamma = 1.0):\n",
    "    \"\"\" Policy-Iteration algorithm \"\"\"\n",
    "    policy = np.random.choice(env.nA, size=(env.nS))  # initialize a random policy\n",
    "    max_iterations = 200000\n",
    "    gamma = 1.0\n",
    "    for i in range(max_iterations):\n",
    "        old_policy_v = compute_policy_v(env, policy, gamma)\n",
    "        new_policy = extract_policy(old_policy_v, gamma)\n",
    "        if (np.all(policy == new_policy)):\n",
    "            print ('Policy-Iteration converged at step %d.' %(i+1))\n",
    "            break\n",
    "        policy = new_policy\n",
    "    return policy\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    env_name  = 'FrozenLake8x8-v0'\n",
    "    env = gym.make(env_name)\n",
    "    env = env.unwrapped\n",
    "    start = time()\n",
    "    optimal_policy = policy_iteration(env, gamma = 1.0)\n",
    "    scores = evaluate_policy(env, optimal_policy, gamma = 1.0)\n",
    "    end = time()\n",
    "    print('Average scores = ', np.mean(scores))\n",
    "    print('Time = {}'.format(end - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gcikBq6BHRQM"
   },
   "source": [
    "<h2><b>Value Iteration</b></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gHvcnTDcHGmH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value-iteration converged at iteration# 2357.\n",
      "Policy average score =  1.0\n",
      "Time = 1.9960312843322754\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Solving FrozenLake8x8 environment using Value-Itertion.\n",
    "Author : Moustafa Alzantot (malzantot@ucla.edu)\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import gym\n",
    "from gym import wrappers\n",
    "from time import time\n",
    "\n",
    "def run_episode(env, policy, gamma = 1.0, render = False):\n",
    "    \"\"\" Evaluates policy by using it to run an episode and finding its\n",
    "    total reward.\n",
    "    args:\n",
    "    env: gym environment.\n",
    "    policy: the policy to be used.\n",
    "    gamma: discount factor.\n",
    "    render: boolean to turn rendering on/off.\n",
    "    returns:\n",
    "    total reward: real value of the total reward recieved by agent under policy.\n",
    "    \"\"\"\n",
    "    obs = env.reset()\n",
    "    total_reward = 0\n",
    "    step_idx = 0\n",
    "    while True:\n",
    "        if render:\n",
    "            env.render()\n",
    "        obs, reward, done , _ = env.step(int(policy[obs]))\n",
    "        total_reward += (gamma ** step_idx * reward)\n",
    "        step_idx += 1\n",
    "        if done:\n",
    "            break\n",
    "    return total_reward\n",
    "\n",
    "\n",
    "def evaluate_policy(env, policy, gamma = 1.0,  n = 100):\n",
    "    \"\"\" Evaluates a policy by running it n times.\n",
    "    returns:\n",
    "    average total reward\n",
    "    \"\"\"\n",
    "    scores = [\n",
    "            run_episode(env, policy, gamma = gamma, render = False)\n",
    "            for _ in range(n)]\n",
    "    return np.mean(scores)\n",
    "\n",
    "def extract_policy(v, gamma = 1.0):\n",
    "    \"\"\" Extract the policy given a value-function \"\"\"\n",
    "    policy = np.zeros(env.nS)\n",
    "    for s in range(env.nS):\n",
    "        q_sa = np.zeros(env.action_space.n)\n",
    "        for a in range(env.action_space.n):\n",
    "            for next_sr in env.P[s][a]:\n",
    "                # next_sr is a tuple of (probability, next state, reward, done)\n",
    "                p, s_, r, _ = next_sr\n",
    "                q_sa[a] += (p * (r + gamma * v[s_]))\n",
    "        policy[s] = np.argmax(q_sa)\n",
    "    return policy\n",
    "\n",
    "\n",
    "def value_iteration(env, gamma = 1.0):\n",
    "    \"\"\" Value-iteration algorithm \"\"\"\n",
    "    v = np.zeros(env.nS)  # initialize value-function\n",
    "    max_iterations = 100000\n",
    "    eps = 1e-20\n",
    "    for i in range(max_iterations):\n",
    "        prev_v = np.copy(v)\n",
    "        for s in range(env.nS):\n",
    "            q_sa = [sum([p*(r + prev_v[s_]) for p, s_, r, _ in env.P[s][a]]) for a in range(env.nA)] \n",
    "            v[s] = max(q_sa)\n",
    "        if (np.sum(np.fabs(prev_v - v)) <= eps):\n",
    "            print ('Value-iteration converged at iteration# %d.' %(i+1))\n",
    "            break\n",
    "    return v\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    env_name  = 'FrozenLake8x8-v0'\n",
    "    gamma = 1.0\n",
    "    env = gym.make(env_name)\n",
    "    env = env.unwrapped\n",
    "    start = time()\n",
    "    optimal_v = value_iteration(env, gamma);\n",
    "    policy = extract_policy(optimal_v, gamma)\n",
    "    policy_score = evaluate_policy(env, policy, gamma, n=1000)\n",
    "    end = time()\n",
    "    print('Policy average score = ', policy_score)\n",
    "    print('Time = {}'.format(end - start))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Stochastic_Processes_&_Optimization_in_Machine_Learning_(Lab_6_Markov_Decision_Processes).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
