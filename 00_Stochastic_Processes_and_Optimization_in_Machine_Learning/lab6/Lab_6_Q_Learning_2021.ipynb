{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gHFOjD_VY0ld"
   },
   "source": [
    "<h1>Q-Learning</h1>\n",
    "\n",
    "<p>Στην συγκεκριμένη άσκηση θα μελετήσετε το στοχαστικό αλγόριθμο Q-Learning, χρησιμοποιώντας το έτοιμο πρόγραμμα που σας δίνεται.</p> <p> Στα πλαίσια του παραδείγματος θα εξετάσετε μία υλοποίηση με q-learning σχετικά με το σύστημα αυτόματης οδήγησης ενός ταξί. Στα πλαίσια του προβλήματος αυτού θα πρέπει να ικανοποιούνται τα εξής:</p>\n",
    "<ul>\n",
    "<li>Το ταξί θα πρέπει να αφήνει τον πελάτη στη σωστή θέση</li>\n",
    "<li>Το ταξί να ακολουθεί τη συντομότερη δυνατή διαδρομή</li>\n",
    "<li>Να τηρούνται οι κανόόνες κυκλοφορίας και ασφάλειας των επιβατών</li>\n",
    "</ul>\n",
    "\n",
    "<p>Στα πλαίσια του προβλήματος έχουμε τα εξής χαρακτηριστικά για την επιβράβευση, τις καταστάσεις και τις ενέργειες.</p> \n",
    "\n",
    "<h4>Επιβράβευση</h4>\n",
    "<ul>\n",
    "<li>Θα έχουμε τη μέγιστη επιβράβευση όταν το ταξί αφήνει έναν πελάτη στην επιθυμητή θέση</li>\n",
    "<li>Θα υπάρχει ποινή στην περίπτωση όπου το ταξί αφήσει τον πελάτη σε κάποιο λανθασμένο σημείο</li>\n",
    "<li>Ο agent θα παίρνει μία μικρή σχετικά ποινή στην περίπτωση όπου αργεί να φτάσει στον τελικό προορισμό</li>\n",
    "</ul>\n",
    "\n",
    "<p>Γενικά οι παραπάνω αρχές συνοψίζονται στα εξής: \"Λαμβάνουμε +20 πόντους για μια επιτυχημένη πτώση και χάνουμε 1 πόντο για κάθε χρονικό βήμα που παίρνει. Υπάρχει επίσης ποινή 10 πόντων για παράνομες ενέργειες παραλαβής και αποχώρησης.\"</p>\n",
    "\n",
    "<h4>Πλήθος Καταστάσεων</h4>\n",
    "<img src=\"https://storage.googleapis.com/lds-media/images/Reinforcement_Learning_Taxi_Env.width-1200.png\">\n",
    "<p>Στο παράδειγμά μας έχουμε ένα μικρό χώρο 5*5. Από εκεί και πέρα έχουμε 4 προορισμούς και 5 πιθανές θέσεις του πελάτη (παίρνουμε και την περίπτωση ο πελάτης να είναι ήδη μέσα στο τάξι).</p>\n",
    "<p>Με βάση τα παραπάνω έχουμε 5*5*5*4=500 πιθανές καταστάσεις.</p>\n",
    "\n",
    "<h4>Πλήθος Ενεργειών</h4>\n",
    "<p>Έχουμε έξι ενεργειες για το ταξί, οι οποίες είναι οι εξής:</p>\n",
    "<ul>\n",
    "<li>0=Νότια</li>\n",
    "<li>1=Βόρεια</li>\n",
    "<li>2=Ανατολικά</li>\n",
    "<li>3=Δυτικά</li>\n",
    "<li>4=Επιβίβαση</li>\n",
    "<li>5=Αποβίβαση</li>\n",
    "</ul>\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HOQpQ0DygDzt"
   },
   "source": [
    "<p>Πριν συνεχίσετε στην άσκηση να απαντήσετε στο εξής ερώτημα:</p>\n",
    "\n",
    "<b><p>1. Να περιγράψετε σύντομα τον αλγόριθμο Q-Learning. Σε ποια προβλήματα θεωρείτε ότι ταιριάζει ως τρόπος εκμάθησης η Ενισχυτική Μάθηση (Reinforcement Learning); Ποια είναι η βασική διαφορά του αλγορίθμου Q-Learning από τους αλγορίθμους Policy Iteration και Value Iteration;</p></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ο αλγόριθμος Q-Learning οφείλει το όνομά του στα Q-factors. Αυτά είναι οι τιμές για όλα τα πιθανά ζεύγη καταστάσεων και ενεργειών (s και a αντίστοιχα). Ο αλγόριθμος δουλεύει επαναληπτικά. Αφού αρχικοποιήσουμε τα ζευγάρια τον εκτελούμε για έναν αριθμό επεισοδίων. Σε κάθε επεισόδιο, υπολογίζουμε το νέο $Q(s,a)$ σαν σταθμισμένο μέσο όρο του παλιού και μιας νέας ποσοτήτας. Για τον σκοπό αυτό ορίζουμε το ρυθμό μάθησης $\\alpha$ οπότε έχουμε ότι \n",
    "$$ Q(s,a)_{\\text{new}} = (1-\\alpha)Q(s,a)_{\\text{old}} + \\alpha (r_t + \\gamma\\cdot \\max_a Q(s_{t+1}, a))$$\n",
    "To Reinforcement Learning είναι κατάλληλο για προβλήματα που ο δυναμικός προγραμματισμός αποτυγχάνει. Δηλαδή, δεν είναι γνωστές οι πιθανότητες μετάβασης, δεν είναι σταθερά τα rewards ή απλά ο χώρος καταστάσεων είναι πολύ μεγάλος για να τον χωρέσουμε στην μνήμη(βλ. σκάκι). \n",
    "Η βασική διαφορά του Q-Learning από τις άλλες δύο μεθόδους είναι ακριβώς το ότι δεν γνωρίζει τις πιθανότητες μετάβασης."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pc11bCDZgfSx"
   },
   "source": [
    "<p>Στη συνέχεια θα πρέπει να φορτώσετε τη βιβλιοθήκη gym καθώς και το σχετικό dataset<p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "y_95lIAzWYpp"
   },
   "outputs": [],
   "source": [
    "#!pip install cmake gym[atari] scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "NDHLb5PGWdwr"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : :\u001b[43m \u001b[0m: |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "env = gym.make(\"Taxi-v3\").env\n",
    "\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "396kTtOrW-cO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : :\u001b[43m \u001b[0m: : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "\n",
      "Action Space Discrete(6)\n",
      "State Space Discrete(500)\n"
     ]
    }
   ],
   "source": [
    "env.reset() # reset environment to a new, random state\n",
    "env.render()\n",
    "\n",
    "print(\"Action Space {}\".format(env.action_space))\n",
    "print(\"State Space {}\".format(env.observation_space))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-UC6-XuIhF5_"
   },
   "source": [
    "<p>Παρακάτω ορίζουμε τις συνεταγμένες του ταξί, τη θέση του πελάτη και το σημείο προορισμού</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "nPSOw5CdXFx1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State: 328\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| |\u001b[43m \u001b[0m: | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "state = env.encode(3, 1, 2, 0) # (taxi row, taxi column, passenger index, destination index)\n",
    "print(\"State:\", state)\n",
    "\n",
    "env.s = state\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NsAkQJhchVFy"
   },
   "source": [
    "<p>Παρακάτω είναι η μήτρα επιβράβευσης για το state που ορίσαμε στο προηγούμενο βήμα</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "j7oDbznIXOJo"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [(1.0, 428, -1, False)],\n",
       " 1: [(1.0, 228, -1, False)],\n",
       " 2: [(1.0, 348, -1, False)],\n",
       " 3: [(1.0, 328, -1, False)],\n",
       " 4: [(1.0, 328, -10, False)],\n",
       " 5: [(1.0, 328, -10, False)]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.P[328]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CtROeD1ph6kk"
   },
   "source": [
    "<p> Τρέχουμε το παράδειγμά μας χωρις τη χρήση Q-Learning.</p>\n",
    "\n",
    "<b><p>2. Τα αποτελέσματα είναι ικανοποιητικά; Πως θα μας εξυπηρετούσε η χρήση του Q-Learning;</p></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Τα αποτέλεσματα δεν είναι καθόλου ικανοποιητικά διότι κάναμε τυχαίες κινήσεις και αφήναμε τον πελάτη σε λάθος θέσεις. Με την χρήση του Q-Learning θα μαθαίναμε γρήγορα ότι δεν έπρεπε να τον αφήνουμε οπουδήποτε και εν συνεχεία που πρέπει να τον αφήσουμε και πώς θα πάμε πιο γρήγορα εκεί."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "vKHhcDxVXUFz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timesteps taken: 1388\n",
      "Penalties incurred: 460\n"
     ]
    }
   ],
   "source": [
    "env.s = 328  # set environment to illustration's state\n",
    "\n",
    "epochs = 0\n",
    "penalties, reward = 0, 0\n",
    "\n",
    "frames = [] # for animation\n",
    "\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    action = env.action_space.sample()\n",
    "    state, reward, done, info = env.step(action)\n",
    "\n",
    "    if reward == -10:\n",
    "        penalties += 1\n",
    "    \n",
    "    # Put each rendered frame into dict for animation\n",
    "    frames.append({\n",
    "        'frame': env.render(mode='ansi'),\n",
    "        'state': state,\n",
    "        'action': action,\n",
    "        'reward': reward\n",
    "        }\n",
    "    )\n",
    "\n",
    "    epochs += 1\n",
    "    \n",
    "    \n",
    "print(\"Timesteps taken: {}\".format(epochs))\n",
    "print(\"Penalties incurred: {}\".format(penalties))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aj3s09rsizVm"
   },
   "source": [
    "<p>Τώρα θα προσπαθήσουμε να λύσουμε το πρόβλημά μας με τη χρήση του Q-Learning.</p>\n",
    "\n",
    "<b><p>3. Τι γνωρίζετε για τις παραμέτρους α και γ. Τι θα συμβεί αν έχουν τιμές ίσες με 1;</p></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Το $\\alpha$ συμβολίζει το \"βάρος\" της πρότερης γνώσης. Αν το $\\alpha$ γίνει ίσο με ένα θα έχουμε το πρόβλημα της συνεχούς αντικατάστασης της παλιάς τιμής με την καινούρια, οπότε επί της ουσίας δεν μαθαίνουμε τίποτα.\n",
    "Το $\\gamma$ απ'ότι την άλλη μεριά \"ζυγίζει\" τα rewards. Όσο πιο κοντά στο 0 τόσο πιο πολύ είμαστε ικανοποιήμενοι με το reward που παίρνουμε από την κίνησή μας και αδιαφορούμε αν πηγαίνουμε σε κατάσταση που μελλοντικά δεν θα δώσει άλλα rewards ενώ με $\\gamma$ κοντά στην μονάδα συμβαίνει το αντίθετο. Τέλος, με $\\gamma > 1$ χαλάει η απόδειξη της σύγκλισης (αποκλίνει η γεωμετρική σειρά που εμφανίζεται)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "JJk3NTfcXrrA"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "q_table = np.zeros([env.observation_space.n, env.action_space.n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "Oy2Yg8DTXtHW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 100000\n",
      "Training finished.\n",
      "\n",
      "Wall time: 37.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\"\"\"Training the agent\"\"\"\n",
    "\n",
    "import random\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Hyperparameters\n",
    "alpha = 0.1\n",
    "gamma = 0.6\n",
    "epsilon = 0.1\n",
    "\n",
    "# For plotting metrics\n",
    "all_epochs = []\n",
    "all_penalties = []\n",
    "\n",
    "for i in range(1, 100001):\n",
    "    state = env.reset()\n",
    "\n",
    "    epochs, penalties, reward, = 0, 0, 0\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        if random.uniform(0, 1) < epsilon:\n",
    "            action = env.action_space.sample() # Explore action space\n",
    "        else:\n",
    "            action = np.argmax(q_table[state]) # Exploit learned values\n",
    "\n",
    "        next_state, reward, done, info = env.step(action) \n",
    "        \n",
    "        old_value = q_table[state, action]\n",
    "        next_max = np.max(q_table[next_state])\n",
    "        \n",
    "        new_value = (1 - alpha) * old_value + alpha * (reward + gamma * next_max)\n",
    "        q_table[state, action] = new_value\n",
    "\n",
    "        if reward == -10:\n",
    "            penalties += 1\n",
    "\n",
    "        state = next_state\n",
    "        epochs += 1\n",
    "        \n",
    "    if i % 100 == 0:\n",
    "        clear_output(wait=True)\n",
    "        print(f\"Episode: {i}\")\n",
    "\n",
    "print(\"Training finished.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "dxt4fmvGYBOm"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -2.41031065,  -2.27325184,  -2.40010488,  -2.35509829,\n",
       "       -10.58525062, -11.07145513])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_table[328]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "6W9JE9yOYGgP"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results after 100 episodes:\n",
      "Average timesteps per episode: 12.78\n",
      "Average penalties per episode: 0.0\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Evaluate agent's performance after Q-learning\"\"\"\n",
    "\n",
    "total_epochs, total_penalties = 0, 0\n",
    "episodes = 100\n",
    "\n",
    "for _ in range(episodes):\n",
    "    state = env.reset()\n",
    "    epochs, penalties, reward = 0, 0, 0\n",
    "    \n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        action = np.argmax(q_table[state])\n",
    "        state, reward, done, info = env.step(action)\n",
    "\n",
    "        if reward == -10:\n",
    "            penalties += 1\n",
    "\n",
    "        epochs += 1\n",
    "\n",
    "    total_penalties += penalties\n",
    "    total_epochs += epochs\n",
    "\n",
    "print(f\"Results after {episodes} episodes:\")\n",
    "print(f\"Average timesteps per episode: {total_epochs / episodes}\")\n",
    "print(f\"Average penalties per episode: {total_penalties / episodes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5R7gW1nLj-qE"
   },
   "source": [
    "<b><p>4. Συγκρίνετε τους δύο αλγορίθμους με βάση τις παρακάτω μετρικές</p>\n",
    "<ul>\n",
    "<li>Μέσος αριθμός παραβάσεων ανά επεισόδιο</li>\n",
    "<li>Μέσος αριθμός βημάτων ανά διαδρομή</li>\n",
    "<li>Μέσος αριθμός ανταμοιβών ανά κίνηση</li>\n",
    "</ul>\n",
    "<p>Τις παραπάνω συγκρίσεις να τις κάνετε για 100 επεισόδια.</p>\n",
    "</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results after 100 episodes:\n",
      "Average timesteps per episode: 2341.88\n",
      "Average penalties per episode: 757.68\n"
     ]
    }
   ],
   "source": [
    "episodes = 100\n",
    "total_epochs, total_penalties = 0,0 \n",
    "\n",
    "for _ in range(episodes):\n",
    "    state = env.reset()\n",
    "    penalties, reward, epochs = 0, 0, 0\n",
    "    frames = [] # for animation\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        action = env.action_space.sample()\n",
    "        state, reward, done, info = env.step(action)\n",
    "        if reward == -10:\n",
    "            penalties += 1\n",
    "\n",
    "        # Put each rendered frame into dict for animation\n",
    "        frames.append({\n",
    "            'frame': env.render(mode='ansi'),\n",
    "            'state': state,\n",
    "            'action': action,\n",
    "            'reward': reward\n",
    "            }\n",
    "        )\n",
    "        epochs += 1\n",
    "    total_penalties += penalties\n",
    "    total_epochs += epochs\n",
    "\n",
    "print(\"Results after {} episodes:\".format(episodes))\n",
    "print(\"Average timesteps per episode: {}\".format(total_epochs / episodes))\n",
    "print(\"Average penalties per episode: {}\".format(total_penalties / episodes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Βλέπουμε ότι ο αριθμός των βημάτων ανά επεισόδιο είναι πολύ μεγαλύτερος όταν δεν χρησιμοποιούμε Q-Learning. Επίσης, επειδή εδώ χρησιμοποιούμε επεισόδια χωρίς άνω όριο κινήσεων, δηλαδή ολοκληρώνονται μόνο όταν αποβιβάσουμε επιτυχώς τον πελάτη, τα βήματα ανά διαδρομή ταυτίζονται με τα βήματα ανά επεισόδιο. Επομένως, αναμενόμενα παρατηρούμε πολύ μεγαλύτερο αριθμό. Τέλος, επειδή ανταμοιβή δίνει μόνο η τελευταία κίνηση και στις δύο περιπτώσεις λαμβάνουμε αμέσως ότι ο αριθμός των ανταμοιβών άνα κίνηση ισούται με 1 προς τον αριθμό των βημάτων ανά επεισόδιο οπότε είναι σαφώς μικρότερος όταν δεν υπάρχει το Q-Learning."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Stochastic_Processes_&_Optimization_in_Machine_Learning_(Lab_6_Q_Learning_2021).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
